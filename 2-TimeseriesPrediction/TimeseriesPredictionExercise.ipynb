{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `%t` as an alias for `%timeit`.\n",
      "Created `%%t` as an alias for `%%timeit`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import namedtuple \n",
    "from itertools import groupby\n",
    "\n",
    "import pyprind #pip install pyprind\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%alias_magic t timeit\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips to handle datasets larger then memory\n",
    "- use iterators, never materialize whole set completely in memory\n",
    "- use map, filter, ... and other functools \n",
    "- filter: https://docs.python.org/3.4/library/functions.html?highlight=filter#filter\n",
    "- itertools: https://docs.python.org/3.4/library/itertools.html#module-itertools\n",
    "- for experiments use only partial dataset, let full experiment run over night\n",
    "\n",
    "# Loading data\n",
    "\n",
    "load_data accepts a gziped file. Uncompression is done on the fly.\n",
    "btw: load_data returns an iterator. always when you call next(load_data), the next item is yielded from the iterator. Be careful when passing it around. With list(it_load_data(file.gz)) you can materialize the iterator in memory. You can find more information here: https://docs.python.org/3.4/glossary.html#term-iterator\n",
    "\n",
    "Uncompressed Household: 1G, compressed (gzip -9): 96M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In the following Cell some namedtuples are defined, [Namedtuples](https://docs.python.org/3.4/library/collections.html#collections.namedtuple) are a lazy way to define classes \n",
    "\n",
    "adapt the sourcefile variable to the directory where you put the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Measurement = namedtuple('Measurement', ['ts', 'sid', 'load'])\n",
    "PredictionBase = namedtuple('PredictionBase', ['base', 'actuals', 'slicemin', 'increment', 'currenttime'])\n",
    "ForecastResult = namedtuple('ForecastResult', ['ts', 'actual', 'predicted'])\n",
    "\n",
    "sourcefile = 'data/household.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def it_load_data(filename):\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        header = f.readline() #ts,hh234,load\n",
    "        ts, sensor, load = header.split(',')\n",
    "        \n",
    "        for line in f:\n",
    "            date_str, sensorid_str, load_str = line.strip('\\r\\n').split(',')\n",
    "            yield Measurement(datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S'), sensorid_str.strip(), int(load_str))\n",
    "\n",
    "def it_take(num, it):\n",
    "    \"\"\"\n",
    "    Returns an iterator which stops after num received items\n",
    "    Displays a pyprind progress bar\n",
    "    \"\"\"\n",
    "    bar = None\n",
    "    for i in range(num):\n",
    "        if bar is None:\n",
    "            bar = pyprind.ProgBar(num)\n",
    "            \n",
    "        yield next(it)\n",
    "        bar.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has some helper functions you might want to use.\n",
    "By convention, each function which begins with _it_ returns an iterator. With list(iterator()) a function can be materialized.\n",
    "The cell below shows the first 4 measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "household_data_samples = list(it_take(4, it_load_data(sourcefile)))\n",
    "household_data_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries prediction\n",
    "The main idea of timeseries prediction is to take n length of history (e.g. 30 minutes) to predict what will happen in the next n minutes of future.\n",
    "\n",
    "## Training and evaluation\n",
    "We need to create a training set and a evaluation set. To train we can take as much historic data as possible to predict the future. In many cases we don't have enough history, also there could be other reasons, e.g. seasonal changes, which would effect the prediction algorithms. Hence in most cases we want to limit the length of history, e.g., 1 month. Machine learning algorithms need a lot of data. We use a simple trick to create more training points from history. We take every increment (e.g., 5min) the last slicemin (e.g. 30 min) to predict the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def it_timeslices(it_measurements, slicemin, increment):\n",
    "    def calc_initial(first, slicemin):\n",
    "        temp = datetime(first.ts.year, first.ts.month, first.ts.day, first.ts.hour, first.ts.minute)\n",
    "        rs = first.ts.minute % (slicemin * 2)\n",
    "        return temp + timedelta(minutes=((slicemin*2) - rs))\n",
    "\n",
    "    timeslice = list()\n",
    "\n",
    "    first = it_measurements.__next__()\n",
    "    timeslice.append(first)\n",
    "    next_ts = calc_initial(first, slicemin)\n",
    "\n",
    "    for meas in it_measurements:\n",
    "        timeslice.append(meas)\n",
    "        while meas.ts >= next_ts: #maybe we left out something and there are intermediate steps ...\n",
    "            predictionbase_begin = next_ts - timedelta(minutes=(slicemin*2))\n",
    "            predictionbase_end = next_ts - timedelta(minutes=slicemin)\n",
    "\n",
    "            learn_base = list()\n",
    "            forecast_base = list()\n",
    "            cnt = 0\n",
    "\n",
    "            for meas in timeslice:\n",
    "                if not meas.ts >= predictionbase_begin:\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    if meas.ts < predictionbase_end:\n",
    "                        learn_base.append(meas)\n",
    "                    elif meas.ts < next_ts:\n",
    "                        forecast_base.append(meas)\n",
    "\n",
    "            timeslice = timeslice[cnt:]\n",
    "            curr = next_ts\n",
    "            next_ts = next_ts + timedelta(minutes=increment)\n",
    "\n",
    "            yield PredictionBase(learn_base, forecast_base, slicemin, increment, (curr - timedelta(minutes=slicemin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display data\n",
    "The column below gets the first 10 slices, 15 min long, every 15 min one slice\n",
    "The datframe displayes from the prediction base the first 10 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itslice = it_timeslices(it_load_data(sourcefile), 15, 15)\n",
    "df = pd.DataFrame(next(itslice).actuals, columns=Measurement._fields)\n",
    "#df[df[\"sid\"] == \"s18\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction for machine learning\n",
    "\n",
    "We need to extract features from the prediction base and try to predict a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_average_load(measures):\n",
    "    #The reason why we do this so complicated is that sometimes a single sensor does not send data that often\n",
    "    sorted_measures = sorted(measures, key=lambda meas: meas.sid)\n",
    "    load_per_device = groupby(sorted_measures, key=lambda meas: meas.sid)\n",
    "    \n",
    "    def get_avg_load_of_device():\n",
    "        for key, gmeasures in load_per_device:\n",
    "            mean = np.mean(list(map(lambda meas: meas.load, gmeasures)))\n",
    "            yield mean\n",
    "            \n",
    "    return np.sum(list(get_avg_load_of_device()))\n",
    "\n",
    "from scipy.stats import kurtosis,skew\n",
    "\n",
    "def extract_features(pb):\n",
    "    dc = dict()\n",
    "    dc.update({\"hour\": pb.currenttime.hour})\n",
    "    dc.update({\"avg\": calc_average_load(pb.base)})\n",
    "    #you can try to add additional features, e.g., last consumption?\n",
    "    sorted_base = sorted(pb.base, key=lambda meas: meas.ts)\n",
    "    max_ts = np.amax(list(map(lambda a: a.ts, sorted_base)))\n",
    "    \n",
    "    load_per_time = groupby(sorted_base, key=lambda meas: meas.ts)\n",
    "    avgs_by_time = []\n",
    "    for key, gmeasures in load_per_time:\n",
    "        l_measures = list(gmeasures)\n",
    "        list_loads = list(map(lambda a: a.load, l_measures))\n",
    "        avgs_by_time.append(np.mean(list_loads))\n",
    "        \n",
    "        if key == max_ts:    \n",
    "            dc.update({\"last\": np.mean(list_loads)})\n",
    "    \n",
    "    dc.update({\"kurtosis\": kurtosis(avgs_by_time)})\n",
    "    dc.update({\"skew\": skew(avgs_by_time)})\n",
    "    #print(sorted_base)\n",
    "    #dc.update({\"last_cons\": np.sum(list(map(lambda a: a.load, list(load_per_time[len(list(load_per_time)) - 1]) )))}) #new feature\n",
    "\n",
    "    \n",
    "    sorted_base = sorted(pb.base, key=lambda meas: meas.sid)\n",
    "    load_per_device = groupby(sorted_base, key=lambda meas: meas.sid)\n",
    "    \n",
    "    for key, gmeasures in load_per_device:\n",
    "        l_measures = list(gmeasures)\n",
    "        list_loads = list(map(lambda a: a.load, l_measures))\n",
    "        dc.update({key + \"_last\": float(l_measures[len(l_measures)-1].load)})\n",
    "        dc.update({key + \"_stdev\": np.std(list_loads)})\n",
    "        dc.update({key + \"_max\": np.amax(list_loads)}) #new feature\n",
    "        dc.update({key + \"_mean\": np.mean(list_loads)}) #new feature\n",
    "        dc.update({key + \"_kurtosis\": kurtosis(list_loads)}) #new feature\n",
    "        dc.update({key + \"_skew\": skew(list_loads)}) #new feature\n",
    "    \n",
    "    dc.update(target=calc_average_load(pb.actuals))\n",
    "    dc.update({'target_disc': 0})\n",
    "    return dc\n",
    "\n",
    "\n",
    "def to_ml_dataset(listdict, discrete):\n",
    "    df = pd.DataFrame(listdict)\n",
    "    tokeep, todrop = 'target_disc', 'target'\n",
    "    if not discrete:\n",
    "        tokeep, todrop = todrop, tokeep\n",
    "        \n",
    "    tmp = df.drop([todrop, tokeep], 1)\n",
    "    valvs = tmp.as_matrix()\n",
    "    target = df[tokeep].values\n",
    "    return np.nan_to_num(valvs), np.nan_to_num(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "The cell below shows some extracted features.\n",
    "\n",
    "Which other features would be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slice_plus_history(it):\n",
    "    history = list()\n",
    "    for sli in it:\n",
    "        history.append(sli)\n",
    "        if(len(history) > 672):\n",
    "            yield sli, history[0]\n",
    "            history = history[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##########] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg</th>\n",
       "      <th>hour</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>last</th>\n",
       "      <th>s06_kurtosis</th>\n",
       "      <th>s06_last</th>\n",
       "      <th>s06_max</th>\n",
       "      <th>s06_mean</th>\n",
       "      <th>s06_skew</th>\n",
       "      <th>s06_stdev</th>\n",
       "      <th>...</th>\n",
       "      <th>s41_stdev</th>\n",
       "      <th>s47_kurtosis</th>\n",
       "      <th>s47_last</th>\n",
       "      <th>s47_max</th>\n",
       "      <th>s47_mean</th>\n",
       "      <th>s47_skew</th>\n",
       "      <th>s47_stdev</th>\n",
       "      <th>skew</th>\n",
       "      <th>target</th>\n",
       "      <th>target_disc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128.791765</td>\n",
       "      <td>0</td>\n",
       "      <td>1.704616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.003769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105</td>\n",
       "      <td>69.970588</td>\n",
       "      <td>-0.984050</td>\n",
       "      <td>43.317898</td>\n",
       "      <td>1.107075</td>\n",
       "      <td>94.716216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94.716216</td>\n",
       "      <td>0</td>\n",
       "      <td>4.397772</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.588798</td>\n",
       "      <td>96.0</td>\n",
       "      <td>472</td>\n",
       "      <td>36.135135</td>\n",
       "      <td>1.742336</td>\n",
       "      <td>52.629332</td>\n",
       "      <td>1.529283</td>\n",
       "      <td>103.707611</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.707611</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555061</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.989873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "      <td>44.867882</td>\n",
       "      <td>0.096403</td>\n",
       "      <td>47.072917</td>\n",
       "      <td>0.997992</td>\n",
       "      <td>227.378818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>227.378818</td>\n",
       "      <td>1</td>\n",
       "      <td>62.829335</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>445.859791</td>\n",
       "      <td>-1.895908</td>\n",
       "      <td>94.0</td>\n",
       "      <td>108</td>\n",
       "      <td>57.340858</td>\n",
       "      <td>-0.292380</td>\n",
       "      <td>49.344771</td>\n",
       "      <td>7.227127</td>\n",
       "      <td>82.547188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82.547188</td>\n",
       "      <td>1</td>\n",
       "      <td>2.544584</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.642530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94</td>\n",
       "      <td>23.331096</td>\n",
       "      <td>1.165091</td>\n",
       "      <td>40.592397</td>\n",
       "      <td>1.509998</td>\n",
       "      <td>141.808082</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>141.808082</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.396100</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.730633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278</td>\n",
       "      <td>82.660674</td>\n",
       "      <td>-1.356519</td>\n",
       "      <td>36.953106</td>\n",
       "      <td>0.423364</td>\n",
       "      <td>66.602213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.602213</td>\n",
       "      <td>1</td>\n",
       "      <td>52.589100</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.972691</td>\n",
       "      <td>106.0</td>\n",
       "      <td>447</td>\n",
       "      <td>7.872483</td>\n",
       "      <td>6.736326</td>\n",
       "      <td>33.365903</td>\n",
       "      <td>4.248270</td>\n",
       "      <td>132.836000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>132.836000</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.230348</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.433933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106</td>\n",
       "      <td>74.171625</td>\n",
       "      <td>-1.233901</td>\n",
       "      <td>41.078039</td>\n",
       "      <td>0.720448</td>\n",
       "      <td>86.719477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86.719477</td>\n",
       "      <td>2</td>\n",
       "      <td>11.236748</td>\n",
       "      <td>39.500000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.318184</td>\n",
       "      <td>99.0</td>\n",
       "      <td>523</td>\n",
       "      <td>27.838636</td>\n",
       "      <td>2.789390</td>\n",
       "      <td>51.127949</td>\n",
       "      <td>2.236294</td>\n",
       "      <td>213.666654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>213.666654</td>\n",
       "      <td>2</td>\n",
       "      <td>38.916813</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>427.818939</td>\n",
       "      <td>-1.956871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99</td>\n",
       "      <td>52.296651</td>\n",
       "      <td>-0.199573</td>\n",
       "      <td>47.299899</td>\n",
       "      <td>6.139343</td>\n",
       "      <td>115.108080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          avg  hour   kurtosis       last  s06_kurtosis  s06_last  s06_max  \\\n",
       "0  128.791765     0   1.704616   0.000000          -3.0       0.0        0   \n",
       "1   94.716216     0   4.397772  14.250000          -3.0       0.0        0   \n",
       "2  103.707611     0   0.555061  12.500000          -3.0       0.0        0   \n",
       "3  227.378818     1  62.829335  27.000000          -3.0       0.0        0   \n",
       "4   82.547188     1   2.544584   2.800000          -3.0       0.0        0   \n",
       "5  141.808082     1  -0.396100   4.666667          -3.0       0.0        0   \n",
       "6   66.602213     1  52.589100  35.333333          -3.0       0.0        0   \n",
       "7  132.836000     2  -0.230348  14.500000          -3.0       0.0        0   \n",
       "8   86.719477     2  11.236748  39.500000          -3.0       0.0        0   \n",
       "9  213.666654     2  38.916813   3.500000          -3.0       0.0        0   \n",
       "\n",
       "   s06_mean  s06_skew  s06_stdev     ...        s41_stdev  s47_kurtosis  \\\n",
       "0       0.0       0.0        0.0     ...         0.000000     -1.003769   \n",
       "1       0.0       0.0        0.0     ...         0.000000      8.588798   \n",
       "2       0.0       0.0        0.0     ...         0.000000     -1.989873   \n",
       "3       0.0       0.0        0.0     ...       445.859791     -1.895908   \n",
       "4       0.0       0.0        0.0     ...         0.000000     -0.642530   \n",
       "5       0.0       0.0        0.0     ...         0.000000      2.730633   \n",
       "6       0.0       0.0        0.0     ...         0.000000     68.972691   \n",
       "7       0.0       0.0        0.0     ...         0.000000     -0.433933   \n",
       "8       0.0       0.0        0.0     ...         0.000000     18.318184   \n",
       "9       0.0       0.0        0.0     ...       427.818939     -1.956871   \n",
       "\n",
       "   s47_last  s47_max   s47_mean  s47_skew  s47_stdev      skew      target  \\\n",
       "0       0.0      105  69.970588 -0.984050  43.317898  1.107075   94.716216   \n",
       "1      96.0      472  36.135135  1.742336  52.629332  1.529283  103.707611   \n",
       "2       0.0       96  44.867882  0.096403  47.072917  0.997992  227.378818   \n",
       "3      94.0      108  57.340858 -0.292380  49.344771  7.227127   82.547188   \n",
       "4       0.0       94  23.331096  1.165091  40.592397  1.509998  141.808082   \n",
       "5       0.0      278  82.660674 -1.356519  36.953106  0.423364   66.602213   \n",
       "6     106.0      447   7.872483  6.736326  33.365903  4.248270  132.836000   \n",
       "7       0.0      106  74.171625 -1.233901  41.078039  0.720448   86.719477   \n",
       "8      99.0      523  27.838636  2.789390  51.127949  2.236294  213.666654   \n",
       "9       0.0       99  52.296651 -0.199573  47.299899  6.139343  115.108080   \n",
       "\n",
       "   target_disc  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "\n",
       "[10 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itslice = it_timeslices(it_load_data(sourcefile), 15, 15)\n",
    "first_ten_extracted = list(it_take(10, map(extract_features, itslice)))\n",
    "df = pd.DataFrame(first_ten_extracted)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization\n",
    "\n",
    "Some machine learning algorithms cannot deal with continous values, hence we have to discretize upfront. There is a huge number of potential ways to discretize load.\n",
    "\n",
    "KMeans needs a number of 'clusters', e.g., it should create 5 clusters\n",
    "The result is put in the column target_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn import mixture\n",
    "\n",
    "def discretize_target(n_clusters, list_of_features):\n",
    "    targets = list(map(lambda f: f['target'], list_of_features))\n",
    "    targets_arr = np.array(targets, dtype=np.float).reshape(-1, 1)\n",
    "    \n",
    "    #You could try a different cluster algorithm here\n",
    "    #km = KMeans(n_clusters = n_clusters)   #little slower then Mini b # 28% MAPE\n",
    "    #km = MiniBatchKMeans(n_clusters = n_clusters)# 28%\n",
    "    km = mixture.GaussianMixture(n_components=n_clusters,covariance_type=\"full\") \n",
    "    km.fit(targets_arr)\n",
    "    \n",
    "    for feature in list_of_features:\n",
    "        feature['target_disc'] = km.predict(feature['target'])[0]\n",
    "    \n",
    "    return km\n",
    "\n",
    "def undiscretize_target(km, value):\n",
    "    #return km.cluster_centers_[value][0]\n",
    "    return km.means_[value][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centroids = discretize_target(3, first_ten_extracted)\n",
    "pd.DataFrame(first_ten_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_persistence(historylength_days, it_slices, minutes, increment, shoulddiscretize=True):\n",
    "    historyslices = historylength_days*24*60/increment\n",
    "    skipintermediate = np.ceil(minutes/increment)\n",
    "    training_historic = list()\n",
    "    intermediate = list()\n",
    "    \n",
    "    one_day_holder = 0\n",
    "    pred_count = False\n",
    "    \n",
    "    cnt = 0\n",
    "    for sli in it_slices:\n",
    "        cnt += 1\n",
    "        \n",
    "        temp_target = calc_average_load(sli.actuals)\n",
    "        intermediate.insert(0, temp_target) #we don't want any overlap of historic and prediction data\n",
    "        if len(intermediate) > skipintermediate: \n",
    "            training_historic.insert(0, intermediate.pop())\n",
    "        \n",
    "        if len(training_historic) > historyslices:\n",
    "            training_historic.pop()\n",
    "            \n",
    "        if cnt > (historyslices + skipintermediate):            \n",
    "            try:\n",
    "                if not pred_count:\n",
    "                    one_day_holder = temp_target\n",
    "                    pred_count = True\n",
    "                    last_price = training_historic[0]\n",
    "                    yield ForecastResult(sli.currenttime, one_day_holder, last_price)\n",
    "                    continue\n",
    "                work_actual = temp_target\n",
    "                predicted = one_day_holder\n",
    "                one_day_holder = work_actual\n",
    "                #print(\"this is work actual : \" + str(work_actual))\n",
    "                #print(\"this is predicted : \" + str(predicted))\n",
    "                #print(\"Done...\")\n",
    "                yield ForecastResult(sli.currenttime, work_actual, predicted)\n",
    "            except Exception as e: \n",
    "                print(\"Error occured-continuing: \" + str(e))\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Prediction using SVR\n",
    "The cell below above shows how to use SVR (a kind of Support Vector Machines) (for more information look here: C-Support Vector Classification)\n",
    "[sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "\n",
    "The plot below shows a prediction done every 15 minutes. Each time we predict based on last 30 minutes what the next 30 minutes will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step by step\n",
    "\n",
    "## First we get a timeslice\n",
    "#a = it_timeslices(it_load_data(sourcefile), 30,30)\n",
    "\n",
    "## The prediction splits it into a training and test dataset and converts it into a matrix\n",
    "#to_ml_dataset([extract_features(next(a))], True)\n",
    "\n",
    "## creates the forecastresult\n",
    "#b = calculate_prediction(1, a, 30, 30)\n",
    "#next(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%t\n",
    "#discretize = True\n",
    "\n",
    "#pred_results = it_take(1000, calculate_prediction(historylength1, it_timeslices(it_load_data(sourcefile), horizon1, increment1), horizon1, increment1))\n",
    "#pred_results_df = pd.DataFrame(list(pred_results), columns=ForecastResult._fields)\n",
    "#del pred_results\n",
    "#pred_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(eval_mape(pred_results))\n",
    "#pred_results_df.set_index('ts').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pred_results_df[600:650].set_index('ts').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The cell below shows how to evaluate the MAPE.\n",
    "The version below also only consumes an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_mape(results):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for f in results:\n",
    "        cnt += 1\n",
    "        sum += np.abs((f.actual - f.predicted) / f.actual)\n",
    "    if cnt == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sum * 100 / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def nrmse(result_list):\n",
    "    \"\"\"\n",
    "    Returns None if result_list is empty or squared sum of actual values is zero\n",
    "    \"\"\"\n",
    "    error_sqr_sum = real_sqr_sum = 0\n",
    "    n = len(result_list)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    for r in result_list:\n",
    "        error_sqr_sum += (r.actual - r.predicted) ** 2\n",
    "        real_sqr_sum  += r.actual ** 2\n",
    "    num = sqrt(error_sqr_sum / n)\n",
    "    denom = sqrt(real_sqr_sum / n)\n",
    "    if denom == 0.0:\n",
    "        return None # or set denom to 0.00001 ???\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn import mixture\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "def calculate_prediction(historylength_days, it_slices, minutes, increment,num_of_class, algorithm,shoulddiscretize=True):\n",
    "    historyslices = historylength_days*24*60/increment\n",
    "    skipintermediate = np.ceil(minutes/increment)\n",
    "    training_historic = list()\n",
    "    intermediate = list()\n",
    "            \n",
    "    cnt = 0\n",
    "    for sli in it_slices:\n",
    "        cnt += 1\n",
    "        \n",
    "        \n",
    "        extracted_features = extract_features(sli)\n",
    "        intermediate.insert(0, extracted_features) #we don't want any overlap of historic and prediction data\n",
    "        if len(intermediate) > skipintermediate: \n",
    "            training_historic.insert(0, intermediate.pop())\n",
    "        \n",
    "        if len(training_historic) > historyslices:\n",
    "            training_historic.pop()\n",
    "            \n",
    "        if cnt > (historyslices + skipintermediate):\n",
    "            if shoulddiscretize: #The centroids can change over time, this is why rediscreize each time\n",
    "                km = discretize_target(num_of_class, training_historic) #changed the discretize_target clusters\n",
    "                            \n",
    "            Xtrain, Ytrain = to_ml_dataset(training_historic, shoulddiscretize) #trainingset\n",
    "            Xtest, Ytest = to_ml_dataset([extracted_features], shoulddiscretize) #predictionset\n",
    "            \n",
    "            #yield training_historic, extracted_features\n",
    "            \n",
    "            try:\n",
    "                #predictor = ensemble.AdaBoostRegressor(loss=\"square\")\n",
    "                \n",
    "                clf = algorithm.fit(Xtrain, Ytrain) #actually training the machine learning algorithm\n",
    "                predicted = clf.predict(Xtest)[0]\n",
    "                if shoulddiscretize:\n",
    "                    predicted = undiscretize_target(km, predicted)\n",
    "                \n",
    "                work_actual = extracted_features['target']\n",
    "                yield ForecastResult(sli.currenttime, work_actual, predicted)\n",
    "            except Exception as e: \n",
    "                print(\"Error occured-continuing: \" + str(e))\n",
    "                pass\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble,tree\n",
    "from sklearn import gaussian_process\n",
    "from sklearn import kernel_ridge,svm\n",
    "from sklearn import linear_model, neighbors,neural_network\n",
    "solution_algo_ensemble = [\n",
    "                 ensemble.ExtraTreesRegressor(),#49\n",
    "                ensemble.GradientBoostingRegressor(),#50\n",
    "                 ensemble.RandomForestRegressor(),#52\n",
    "                ]\n",
    "solution_algo_gaus = [\n",
    "    gaussian_process.GaussianProcessRegressor(),#100\n",
    "    \n",
    "]\n",
    "sol_alg_kernrig = [\n",
    "    kernel_ridge.KernelRidge()#354\n",
    "]\n",
    "sol_lin_model = [\n",
    "    #linear_model.ARDRegression(),#90\n",
    "    #linear_model.BayesianRidge(), # 47 - 0.70\n",
    "    #linear_model.ElasticNet(), # 185 - 4\n",
    "    #linear_model.HuberRegressor(), # 43 - 0.76\n",
    "    #linear_model.Lars(), # worst ever\n",
    "    #linear_model.Lasso(),# 257 - 4\n",
    "    linear_model.LassoLars(),# 63 - 0.80\n",
    "    linear_model.LassoLarsIC(),# 51 - 0.66\n",
    "    linear_model.LinearRegression(),# 1080 - 30\n",
    "    linear_model.PassiveAggressiveRegressor(), # 75 - 0.93\n",
    "    linear_model.RANSACRegressor(), # problem none...\n",
    "    linear_model.Ridge(),# 423 - 10 \n",
    "    linear_model.SGDRegressor(), # second worst\n",
    "    linear_model.TheilSenRegressor() # 4000\n",
    "]\n",
    "sol_neigh = [\n",
    "    neighbors.KNeighborsRegressor(), # 37 - 0.66\n",
    "    neighbors.RadiusNeighborsRegressor(), # nan\n",
    "]\n",
    "sol_nn = [\n",
    "    neural_network.MLPRegressor() # 70 - 0.90\n",
    "]\n",
    "sol_neigh_class = [\n",
    "    neighbors.KNeighborsClassifier() # 56 - 0.87\n",
    "]\n",
    "sol_svm = [\n",
    "    svm.LinearSVR(), # 92 - 1.28\n",
    "    svm.NuSVR(), # 29 - 0.67\n",
    "    svm.SVR(), # 27 - 0.68 # hor 200, 7, 50, --> 28 - 0.62\n",
    "    #  svr continue \n",
    "    #  300 7 50  --> 32 - 0.57\n",
    "    #  300 7 100 --> 32 - 0.56\n",
    "    #  400 7 100 --> 32 - 0.50\n",
    "    #  500 4 300 --> 32 - 0.47\n",
    "    #  600 4 400 --> 31 - 0.42\n",
    "    #  800 4 400 --> 29 - 0.39\n",
    "    #  800 2 400 --> 28 - 0.38\n",
    "    # 1000 2 400 --> 28 - 0.34\n",
    "    # 1000 2 600 --> 28 - 0.33    ---> benchmark\n",
    "    # 1000 2 1000--> 34 - 0.38   ---> 37 - 0.45\n",
    "    # 1200 2 1200--> 37 - 0.38   ---> 37 - 0.45\n",
    "    #  600 2 600 --> 32 - 0.48  ---> 62 - 0.70\n",
    "]\n",
    "sol_tree = [\n",
    "    tree.DecisionTreeRegressor(), # 57 - 0.80\n",
    "    tree.ExtraTreeRegressor() # 58 - 0.85\n",
    "]\n",
    "sol_alg = svm.SVR(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "horizon1 = 1000 \n",
    "historylength1 = 2\n",
    "increment1 = 1000\n",
    "num_discrete_class = 20\n",
    "discretize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [####################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE of the prediction was: 34.27711873285752\n",
      "The NRMSE of the prediction was: 0.38725845474187964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:02:22\n",
      "0% [####################] 100% | ETA: 00:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAPE of the benchmark Persistence with same parameters was: 37.86072414878426\n",
      "The NRMSE of the benchmark Persistence with same parameters was: 0.45097129819127957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time elapsed: 00:01:27\n"
     ]
    }
   ],
   "source": [
    "def prediction_evaluation(horizon_and_predbase, historylength, increment, num_of_class,discretize,algorithm):\n",
    "    evaluation_length = int(2 * 7 * 24 * 60 /increment)\n",
    "    #evaluation_length = 100\n",
    "    res_list = list(it_take(evaluation_length, calculate_prediction(historylength, it_timeslices(it_load_data(sourcefile), horizon_and_predbase, increment), horizon_and_predbase, increment,num_of_class,algorithm,discretize)))\n",
    "    #res_list = list(it_take(evaluation_length, calculate_persistence(historylength, it_timeslices(it_load_data(sourcefile), horizon_and_predbase, increment), horizon_and_predbase, increment)))\n",
    "    return eval_mape(res_list),nrmse(res_list), res_list\n",
    "\n",
    "def benchmark_evaluation(horizon_and_predbase, historylength, increment, num_of_class,discretize,algorithm):\n",
    "    evaluation_length = int(2 * 7 * 24 * 60 /increment)\n",
    "    res_list = list(it_take(evaluation_length, calculate_persistence(historylength, it_timeslices(it_load_data(sourcefile), horizon_and_predbase, increment), horizon_and_predbase, increment)))\n",
    "    return eval_mape(res_list),nrmse(res_list), res_list\n",
    "\n",
    "\n",
    "#for sol_alg in sol_tree    \n",
    "percent_mape,percent_nrmse,pred_results  = prediction_evaluation(horizon1, historylength1, increment1,num_discrete_class,discretize,sol_alg) #Evaluate [horizon1] ahead forecast, [historylength1] days of learning, every [increment1] minutes\n",
    "print(\"The MAPE of the prediction was:\", percent_mape)\n",
    "print(\"The NRMSE of the prediction was:\", percent_nrmse)\n",
    "pred_results_df = pd.DataFrame(list(pred_results), columns=ForecastResult._fields)\n",
    "\n",
    "# benchmark \n",
    "# 1000 2 1000 ---> 37 0.45\n",
    "\n",
    "pred_results_df = pd.DataFrame(list(pred_results), columns=ForecastResult._fields)\n",
    "percent_mape_bench,percent_nrmse_bench,bench_results  = benchmark_evaluation(horizon1, historylength1, increment1,num_discrete_class,discretize,sol_alg) #Evaluate [horizon1] ahead forecast, [historylength1] days of learning, every [increment1] minutes\n",
    "print(\"The MAPE of the benchmark Persistence with same parameters was:\", percent_mape_bench)\n",
    "print(\"The NRMSE of the benchmark Persistence with same parameters was:\", percent_nrmse_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(percent_mape_bench)\n",
    "pred_results_df = pd.DataFrame(list(bench_results), columns=ForecastResult._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_results_df.set_index('ts').plot()\n",
    "len(pred_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
