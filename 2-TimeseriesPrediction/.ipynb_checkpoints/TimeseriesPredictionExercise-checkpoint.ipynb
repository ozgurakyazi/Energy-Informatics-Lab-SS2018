{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import namedtuple \n",
    "from itertools import groupby\n",
    "\n",
    "import pyprind #pip install pyprind\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%alias_magic t timeit\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips to handle datasets larger then memory\n",
    "- use iterators, never materialize whole set completely in memory\n",
    "- use map, filter, ... and other functools \n",
    "- filter: https://docs.python.org/3.4/library/functions.html?highlight=filter#filter\n",
    "- itertools: https://docs.python.org/3.4/library/itertools.html#module-itertools\n",
    "- for experiments use only partial dataset, let full experiment run over night\n",
    "\n",
    "# Loading data\n",
    "\n",
    "load_data accepts a gziped file. Uncompression is done on the fly.\n",
    "btw: load_data returns an iterator. always when you call next(load_data), the next item is yielded from the iterator. Be careful when passing it around. With list(it_load_data(file.gz)) you can materialize the iterator in memory. You can find more information here: https://docs.python.org/3.4/glossary.html#term-iterator\n",
    "\n",
    "Uncompressed Household: 1G, compressed (gzip -9): 96M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In the following Cell some namedtuples are defined, [Namedtuples](https://docs.python.org/3.4/library/collections.html#collections.namedtuple) are a lazy way to define classes \n",
    "\n",
    "adapt the sourcefile variable to the directory where you put the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measurement = namedtuple('Measurement', ['ts', 'sid', 'load'])\n",
    "PredictionBase = namedtuple('PredictionBase', ['base', 'actuals', 'slicemin', 'increment', 'currenttime'])\n",
    "ForecastResult = namedtuple('ForecastResult', ['ts', 'actual', 'predicted'])\n",
    "\n",
    "sourcefile = 'data/household.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def it_load_data(filename):\n",
    "    with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
    "        header = f.readline() #ts,hh234,load\n",
    "        ts, sensor, load = header.split(',')\n",
    "        \n",
    "        for line in f:\n",
    "            date_str, sensorid_str, load_str = line.strip('\\r\\n').split(',')\n",
    "            yield Measurement(datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S'), sensorid_str.strip(), int(load_str))\n",
    "\n",
    "def it_take(num, it):\n",
    "    \"\"\"\n",
    "    Returns an iterator which stops after num received items\n",
    "    Displays a pyprind progress bar\n",
    "    \"\"\"\n",
    "    bar = None\n",
    "    for i in range(num):\n",
    "        if bar is None:\n",
    "            bar = pyprind.ProgBar(num)\n",
    "            \n",
    "        yield next(it)\n",
    "        bar.update()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above has some helper functions you might want to use.\n",
    "By convention, each function which begins with _it_ returns an iterator. With list(iterator()) a function can be materialized.\n",
    "The cell below shows the first 4 measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_data_samples = list(it_take(4, it_load_data(sourcefile)))\n",
    "household_data_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries prediction\n",
    "The main idea of timeseries prediction is to take n length of history (e.g. 30 minutes) to predict what will happen in the next n minutes of future.\n",
    "\n",
    "## Training and evaluation\n",
    "We need to create a training set and a evaluation set. To train we can take as much historic data as possible to predict the future. In many cases we don't have enough history, also there could be other reasons, e.g. seasonal changes, which would effect the prediction algorithms. Hence in most cases we want to limit the length of history, e.g., 1 month. Machine learning algorithms need a lot of data. We use a simple trick to create more training points from history. We take every increment (e.g., 5min) the last slicemin (e.g. 30 min) to predict the future.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def it_timeslices(it_measurements, slicemin, increment):\n",
    "    def calc_initial(first, slicemin):\n",
    "        temp = datetime(first.ts.year, first.ts.month, first.ts.day, first.ts.hour, first.ts.minute)\n",
    "        rs = first.ts.minute % (slicemin * 2)\n",
    "        return temp + timedelta(minutes=((slicemin*2) - rs))\n",
    "\n",
    "    timeslice = list()\n",
    "\n",
    "    first = it_measurements.__next__()\n",
    "    timeslice.append(first)\n",
    "    next_ts = calc_initial(first, slicemin)\n",
    "\n",
    "    for meas in it_measurements:\n",
    "        timeslice.append(meas)\n",
    "        while meas.ts >= next_ts: #maybe we left out something and there are intermediate steps ...\n",
    "            predictionbase_begin = next_ts - timedelta(minutes=(slicemin*2))\n",
    "            predictionbase_end = next_ts - timedelta(minutes=slicemin)\n",
    "\n",
    "            learn_base = list()\n",
    "            forecast_base = list()\n",
    "            cnt = 0\n",
    "\n",
    "            for meas in timeslice:\n",
    "                if not meas.ts >= predictionbase_begin:\n",
    "                    cnt += 1\n",
    "                else:\n",
    "                    if meas.ts < predictionbase_end:\n",
    "                        learn_base.append(meas)\n",
    "                    elif meas.ts < next_ts:\n",
    "                        forecast_base.append(meas)\n",
    "\n",
    "            timeslice = timeslice[cnt:]\n",
    "            curr = next_ts\n",
    "            next_ts = next_ts + timedelta(minutes=increment)\n",
    "\n",
    "            yield PredictionBase(learn_base, forecast_base, slicemin, increment, (curr - timedelta(minutes=slicemin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display data\n",
    "The column below gets the first 10 slices, 15 min long, every 15 min one slice\n",
    "The datframe displayes from the prediction base the first 10 lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itslice = it_timeslices(it_load_data(sourcefile), 15, 15)\n",
    "df = pd.DataFrame(next(itslice).actuals, columns=Measurement._fields)\n",
    "#df[df[\"sid\"] == \"s18\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction for machine learning\n",
    "\n",
    "We need to extract features from the prediction base and try to predict a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_average_load(measures):\n",
    "    #The reason why we do this so complicated is that sometimes a single sensor does not send data that often\n",
    "    sorted_measures = sorted(measures, key=lambda meas: meas.sid)\n",
    "    load_per_device = groupby(sorted_measures, key=lambda meas: meas.sid)\n",
    "    \n",
    "    def get_avg_load_of_device():\n",
    "        for key, gmeasures in load_per_device:\n",
    "            mean = np.mean(list(map(lambda meas: meas.load, gmeasures)))\n",
    "            yield mean\n",
    "            \n",
    "    return np.sum(list(get_avg_load_of_device()))\n",
    "\n",
    "def extract_features(pb):\n",
    "    dc = dict()\n",
    "    dc.update({\"hour\": pb.currenttime.hour})\n",
    "    dc.update({\"avg\": calc_average_load(pb.base)})\n",
    "    #you can try to add additional features, e.g., last consumption?\n",
    "    \n",
    "    sorted_base = sorted(pb.base, key=lambda meas: meas.sid)\n",
    "    load_per_device = groupby(sorted_base, key=lambda meas: meas.sid)\n",
    "    \n",
    "    for key, gmeasures in load_per_device:\n",
    "        l_measures = list(gmeasures)\n",
    "        dc.update({key + \"_last\": float(l_measures[len(l_measures)-1].load)})\n",
    "        dc.update({key + \"_stdev\": np.std(list(map(lambda a: a.load, l_measures)))})\n",
    "        dc.update({key + \"_max\": np.amax(list(map(lambda a: a.load, l_measures)))}) #new feature\n",
    "    \n",
    "    dc.update(target=calc_average_load(pb.actuals))\n",
    "    dc.update({'target_disc': 0})\n",
    "    return dc\n",
    "\n",
    "\n",
    "def to_ml_dataset(listdict, discrete):\n",
    "    df = pd.DataFrame(listdict)\n",
    "    tokeep, todrop = 'target_disc', 'target'\n",
    "    if not discrete:\n",
    "        tokeep, todrop = todrop, tokeep\n",
    "        \n",
    "    tmp = df.drop([todrop, tokeep], 1)\n",
    "    valvs = tmp.as_matrix()\n",
    "    target = df[tokeep].values\n",
    "    return np.nan_to_num(valvs), np.nan_to_num(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "The cell below shows some extracted features.\n",
    "\n",
    "Which other features would be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_plus_history(it):\n",
    "    history = list()\n",
    "    for sli in it:\n",
    "        history.append(sli)\n",
    "        if(len(history) > 672):\n",
    "            yield sli, history[0]\n",
    "            history = history[1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itslice = it_timeslices(it_load_data(sourcefile), 15, 15)\n",
    "first_ten_extracted = list(it_take(10, map(extract_features, itslice)))\n",
    "df = pd.DataFrame(first_ten_extracted)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization\n",
    "\n",
    "Some machine learning algorithms cannot deal with continous values, hence we have to discretize upfront. There is a huge number of potential ways to discretize load.\n",
    "\n",
    "KMeans needs a number of 'clusters', e.g., it should create 5 clusters\n",
    "The result is put in the column target_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "from sklearn import mixture\n",
    "\n",
    "def discretize_target(n_clusters, list_of_features):\n",
    "    targets = list(map(lambda f: f['target'], list_of_features))\n",
    "    targets_arr = np.array(targets, dtype=np.float).reshape(-1, 1)\n",
    "    \n",
    "    #You could try a different cluster algorithm here\n",
    "    km = KMeans(n_clusters = n_clusters)   #little slower then Mini b # 28% MAPE\n",
    "    #km = MiniBatchKMeans(n_clusters = n_clusters)# 28%\n",
    "    #km = mixture.GaussianMixture(n_components=n_clusters,covariance_type=\"diag\") #27%\n",
    "    km.fit(targets_arr)\n",
    "    \n",
    "    for feature in list_of_features:\n",
    "        feature['target_disc'] = km.predict(feature['target'])[0]\n",
    "    \n",
    "    return km\n",
    "\n",
    "def undiscretize_target(km, value):\n",
    "    return km.cluster_centers_[value][0]\n",
    "    #return km.means_[value][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = discretize_target(3, first_ten_extracted)\n",
    "pd.DataFrame(first_ten_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import linear_model\n",
    "from sklearn import mixture\n",
    "from sklearn import tree\n",
    "\n",
    "def evaluate_prediction(historylength_days, it_slices, minutes, increment, shoulddiscretize=True):\n",
    "    historyslices = historylength_days*24*60/increment\n",
    "    skipintermediate = np.ceil(minutes/increment)\n",
    "    training_historic = list()\n",
    "    intermediate = list()\n",
    "            \n",
    "    cnt = 0\n",
    "    for sli in it_slices:\n",
    "        cnt += 1\n",
    "        \n",
    "        \n",
    "        extracted_features = extract_features(sli)\n",
    "        intermediate.insert(0, extracted_features) #we don't want any overlap of historic and prediction data\n",
    "        if len(intermediate) > skipintermediate: \n",
    "            training_historic.insert(0, intermediate.pop())\n",
    "        \n",
    "        if len(training_historic) > historyslices:\n",
    "            training_historic.pop()\n",
    "            \n",
    "        if cnt > (historyslices + skipintermediate):\n",
    "            if shoulddiscretize: #The centroids can change over time, this is why rediscreize each time\n",
    "                km = discretize_target(20, training_historic) #changed the discretize_target clusters\n",
    "                            \n",
    "            Xtrain, Ytrain = to_ml_dataset(training_historic, shoulddiscretize) #trainingset\n",
    "            Xtest, Ytest = to_ml_dataset([extracted_features], shoulddiscretize) #predictionset\n",
    "            \n",
    "            #yield training_historic, extracted_features\n",
    "            \n",
    "            try:\n",
    "                #predictor = tree.ExtraTreeClassifier() #you may want to do the training once a day/week to make the prediction faster\n",
    "                predictor = naive_bayes.GaussianNB()\n",
    "                #predictor = naive_bayes.MultinomialNB()\n",
    "                #predictor = naive_bayes.BernoulliNB()\n",
    "                #predictor = linear_model.RidgeClassifier()\n",
    "                #predictor = linear_model.SGDClassifier()   # not good\n",
    "                #predictor = mixture.BayesianGaussianMixture()\n",
    "                clf = predictor.fit(Xtrain, Ytrain) #actually training the machine learning algorithm\n",
    "                predicted = clf.predict(Xtest)[0]\n",
    "                if shoulddiscretize:\n",
    "                    predicted = undiscretize_target(km, predicted)\n",
    "                \n",
    "                work_actual = extracted_features['target']\n",
    "                yield ForecastResult(sli.currenttime, work_actual, predicted)\n",
    "            except Exception as e: \n",
    "                print(\"Error occured-continuing: \" + str(e))\n",
    "                pass\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Prediction using SVR\n",
    "The cell below above shows how to use SVR (a kind of Support Vector Machines) (for more information look here: C-Support Vector Classification)\n",
    "[sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "\n",
    "The plot below shows a prediction done every 15 minutes. Each time we predict based on last 30 minutes what the next 30 minutes will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step by step\n",
    "\n",
    "## First we get a timeslice\n",
    "#a = it_timeslices(it_load_data(sourcefile), 30,30)\n",
    "\n",
    "## The prediction splits it into a training and test dataset and converts it into a matrix\n",
    "#to_ml_dataset([extract_features(next(a))], True)\n",
    "\n",
    "## creates the forecastresult\n",
    "#b = evaluate_prediction(1, a, 30, 30)\n",
    "#next(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%t\n",
    "#discretize = True\n",
    "\n",
    "#pred_results = it_take(1000, evaluate_prediction(historylength1, it_timeslices(it_load_data(sourcefile), horizon1, increment1), horizon1, increment1))\n",
    "#pred_results_df = pd.DataFrame(list(pred_results), columns=ForecastResult._fields)\n",
    "#del pred_results\n",
    "#pred_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eval_mape(pred_results))\n",
    "#pred_results_df.set_index('ts').plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_results_df[600:650].set_index('ts').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "The cell below shows how to evaluate the MAPE.\n",
    "The version below also only consumes an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mape(results):\n",
    "    cnt = 0\n",
    "    sum = 0\n",
    "    for f in results:\n",
    "        cnt += 1\n",
    "        sum += np.abs((f.actual - f.predicted) / f.actual)\n",
    "    if cnt == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sum * 100 / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon1 = 90\n",
    "historylength1 = 2\n",
    "increment1 = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_evaluation(horizon_and_predbase, historylength, increment):\n",
    "    evaluation_length = int(2 * 7 * 24 * 60 /increment)\n",
    "    res_list = list(it_take(evaluation_length, evaluate_prediction(historylength, it_timeslices(it_load_data(sourcefile), horizon_and_predbase, increment), horizon_and_predbase, increment)))\n",
    "    return eval_mape(res_list), res_list\n",
    "\n",
    "percent_mape,pred_results  = prediction_evaluation(horizon1, historylength1, increment1) #Evaluate [horizon1] ahead forecast, [historylength1] days of learning, every [increment1] minutes\n",
    "print(\"The MAPE of the prediction was:\", percent_mape) \n",
    "pred_results_df = pd.DataFrame(list(pred_results), columns=ForecastResult._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results_df[0:50].set_index('ts').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse(list(pred_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def nrmse(result_list):\n",
    "    \"\"\"\n",
    "    Returns None if result_list is empty or squared sum of actual values is zero\n",
    "    \"\"\"\n",
    "    error_sqr_sum = real_sqr_sum = 0\n",
    "    n = len(result_list)\n",
    "    if n == 0:\n",
    "        return None\n",
    "    for r in result_list:\n",
    "        error_sqr_sum += (r.actual - r.predicted) ** 2\n",
    "        real_sqr_sum  += r.actual ** 2\n",
    "    num = sqrt(error_sqr_sum / n)\n",
    "    denom = sqrt(real_sqr_sum / n)\n",
    "    if denom == 0.0:\n",
    "        return None # or set denom to 0.00001 ???\n",
    "    return num / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
